{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIS Auto-Research Engine - Comprehensive Result Analysis\n",
    "\n",
    "This notebook provides in-depth analysis tools for exploring and understanding your experimental results. You'll learn to:\n",
    "- Query and filter the experiment database\n",
    "- Compute statistical summaries and confidence intervals\n",
    "- Compare top-performing configurations\n",
    "- Generate publication-ready visualizations\n",
    "- Perform statistical significance tests\n",
    "- Analyze fidelity gaps and scaling behavior\n",
    "- Export recommendations and best configurations\n",
    "\n",
    "**Prerequisites:** Run some experiments first using `01_quickstart.ipynb` or `03_run_search.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import analysis tools and connect to the results database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Statistical analysis (optional)\n",
    "try:\n",
    "    from scipy import stats\n",
    "    SCIPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SCIPY_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  scipy not available - statistical tests will be skipped\")\n",
    "    print(\"   Install with: pip install scipy\")\n",
    "\n",
    "# RIS Engine imports\n",
    "from ris_research_engine.engine import ResultAnalyzer, ReportGenerator\n",
    "from ris_research_engine.ui import RISEngine\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid' if 'seaborn-v0_8-whitegrid' in plt.style.available else 'default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 7)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Initialize tools\n",
    "analyzer = ResultAnalyzer(db_path=\"results.db\")\n",
    "reporter = ReportGenerator(output_dir=\"outputs\")\n",
    "engine = RISEngine(db_path=\"results.db\", output_dir=\"outputs\")\n",
    "\n",
    "print(\"‚úì Analysis tools initialized\")\n",
    "print(f\"  Database: results.db\")\n",
    "print(f\"  Output directory: outputs/\")\n",
    "print(f\"  Statistical tests: {'Enabled' if SCIPY_AVAILABLE else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Display Experiment Database\n",
    "\n",
    "Query the database to see all completed experiments with their key parameters and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database\n",
    "conn = sqlite3.connect(\"results.db\")\n",
    "\n",
    "# Query all experiments\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    name,\n",
    "    status,\n",
    "    timestamp,\n",
    "    training_time_seconds,\n",
    "    total_epochs,\n",
    "    best_epoch,\n",
    "    model_parameters\n",
    "FROM experiments\n",
    "ORDER BY timestamp DESC\n",
    "\"\"\"\n",
    "\n",
    "experiments_df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Parse JSON fields for display\n",
    "try:\n",
    "    import json\n",
    "    \n",
    "    # Get additional details from JSON columns\n",
    "    detail_query = \"\"\"\n",
    "    SELECT id, config, metrics \n",
    "    FROM experiments\n",
    "    \"\"\"\n",
    "    details_df = pd.read_sql_query(detail_query, conn)\n",
    "    \n",
    "    # Extract probe and model types\n",
    "    probe_types = []\n",
    "    model_types = []\n",
    "    top1_accs = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for _, row in details_df.iterrows():\n",
    "        config = json.loads(row['config'])\n",
    "        metrics = json.loads(row['metrics'])\n",
    "        \n",
    "        probe_types.append(config.get('probe_type', 'N/A'))\n",
    "        model_types.append(config.get('model_type', 'N/A'))\n",
    "        top1_accs.append(metrics.get('top_1_accuracy', 0))\n",
    "        val_losses.append(metrics.get('val_loss', 0))\n",
    "    \n",
    "    experiments_df['probe_type'] = probe_types\n",
    "    experiments_df['model_type'] = model_types\n",
    "    experiments_df['top_1_accuracy'] = top1_accs\n",
    "    experiments_df['val_loss'] = val_losses\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not parse JSON fields: {e}\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"EXPERIMENT DATABASE SUMMARY\")\n",
    "print(\"=\"*90)\n",
    "print(f\"Total Experiments: {len(experiments_df)}\")\n",
    "print(f\"Completed: {sum(experiments_df['status'] == 'completed')}\")\n",
    "print(f\"Failed: {sum(experiments_df['status'] == 'failed')}\")\n",
    "print(f\"Running: {sum(experiments_df['status'] == 'running')}\")\n",
    "\n",
    "if len(experiments_df) > 0:\n",
    "    print(f\"\\nDate Range: {experiments_df['timestamp'].min()} to {experiments_df['timestamp'].max()}\")\n",
    "    print(f\"Total Training Time: {experiments_df['training_time_seconds'].sum():.1f}s ({experiments_df['training_time_seconds'].sum()/3600:.2f}h)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "\n",
    "# Display table (top 10 most recent)\n",
    "display_cols = ['id', 'name', 'status', 'probe_type', 'model_type', \n",
    "                'top_1_accuracy', 'training_time_seconds', 'timestamp']\n",
    "display_cols = [c for c in display_cols if c in experiments_df.columns]\n",
    "\n",
    "if len(experiments_df) > 0:\n",
    "    print(\"\\nMost Recent Experiments:\")\n",
    "    print(experiments_df[display_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No experiments found in database.\")\n",
    "    print(\"   Run some experiments first using 01_quickstart.ipynb or 03_run_search.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Experiments\n",
    "\n",
    "Apply filters to focus on specific subsets of experiments by campaign, status, date, or other criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter configuration\n",
    "FILTER_STATUS = 'completed'  # Options: 'completed', 'failed', 'running', None (all)\n",
    "FILTER_PROBE = None          # Options: specific probe name, None (all)\n",
    "FILTER_MODEL = None          # Options: specific model name, None (all)\n",
    "FILTER_CAMPAIGN = None       # Options: campaign name substring, None (all)\n",
    "\n",
    "# Apply filters\n",
    "filtered_df = experiments_df.copy()\n",
    "\n",
    "if FILTER_STATUS:\n",
    "    filtered_df = filtered_df[filtered_df['status'] == FILTER_STATUS]\n",
    "    print(f\"‚úì Filtered by status: {FILTER_STATUS}\")\n",
    "\n",
    "if FILTER_PROBE and 'probe_type' in filtered_df.columns:\n",
    "    filtered_df = filtered_df[filtered_df['probe_type'] == FILTER_PROBE]\n",
    "    print(f\"‚úì Filtered by probe: {FILTER_PROBE}\")\n",
    "\n",
    "if FILTER_MODEL and 'model_type' in filtered_df.columns:\n",
    "    filtered_df = filtered_df[filtered_df['model_type'] == FILTER_MODEL]\n",
    "    print(f\"‚úì Filtered by model: {FILTER_MODEL}\")\n",
    "\n",
    "if FILTER_CAMPAIGN:\n",
    "    filtered_df = filtered_df[filtered_df['name'].str.contains(FILTER_CAMPAIGN, case=False)]\n",
    "    print(f\"‚úì Filtered by campaign: {FILTER_CAMPAIGN}\")\n",
    "\n",
    "print(f\"\\nFiltered Results: {len(filtered_df)} / {len(experiments_df)} experiments\")\n",
    "\n",
    "if len(filtered_df) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No experiments match the filter criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary\n",
    "\n",
    "Compute aggregated statistics (mean, std, min, max) for each metric across all filtered experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_df) > 0 and 'top_1_accuracy' in filtered_df.columns:\n",
    "    # Group by probe type if available\n",
    "    if 'probe_type' in filtered_df.columns:\n",
    "        grouped = filtered_df.groupby('probe_type').agg({\n",
    "            'top_1_accuracy': ['mean', 'std', 'min', 'max', 'count'],\n",
    "            'val_loss': ['mean', 'std', 'min', 'max'],\n",
    "            'training_time_seconds': ['mean', 'std', 'min', 'max']\n",
    "        }).round(4)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"STATISTICAL SUMMARY BY PROBE TYPE\")\n",
    "        print(\"=\"*90)\n",
    "        print(grouped)\n",
    "        print(\"=\"*90)\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"OVERALL STATISTICS\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    stats_data = {\n",
    "        'Metric': [],\n",
    "        'Mean': [],\n",
    "        'Std Dev': [],\n",
    "        'Min': [],\n",
    "        'Max': [],\n",
    "        'Median': []\n",
    "    }\n",
    "    \n",
    "    for metric in ['top_1_accuracy', 'val_loss', 'training_time_seconds']:\n",
    "        if metric in filtered_df.columns:\n",
    "            values = filtered_df[metric].dropna()\n",
    "            if len(values) > 0:\n",
    "                stats_data['Metric'].append(metric)\n",
    "                stats_data['Mean'].append(f\"{values.mean():.4f}\")\n",
    "                stats_data['Std Dev'].append(f\"{values.std():.4f}\")\n",
    "                stats_data['Min'].append(f\"{values.min():.4f}\")\n",
    "                stats_data['Max'].append(f\"{values.max():.4f}\")\n",
    "                stats_data['Median'].append(f\"{values.median():.4f}\")\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    print(stats_df.to_string(index=False))\n",
    "    print(\"=\"*90)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for statistical summary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Top 5 Experiments\n",
    "\n",
    "Identify and display the best-performing experiments based on top-1 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_df) > 0 and 'top_1_accuracy' in filtered_df.columns:\n",
    "    # Sort by accuracy\n",
    "    top_experiments = filtered_df.nlargest(5, 'top_1_accuracy')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"TOP 5 EXPERIMENTS (by Top-1 Accuracy)\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Display key columns\n",
    "    display_cols = ['id', 'name', 'probe_type', 'model_type', \n",
    "                   'top_1_accuracy', 'val_loss', 'training_time_seconds']\n",
    "    display_cols = [c for c in display_cols if c in top_experiments.columns]\n",
    "    \n",
    "    # Format for display\n",
    "    top_display = top_experiments[display_cols].copy()\n",
    "    for col in ['top_1_accuracy', 'val_loss']:\n",
    "        if col in top_display.columns:\n",
    "            top_display[col] = top_display[col].apply(lambda x: f\"{x:.4f}\")\n",
    "    if 'training_time_seconds' in top_display.columns:\n",
    "        top_display['training_time_seconds'] = top_display['training_time_seconds'].apply(lambda x: f\"{x:.1f}\")\n",
    "    \n",
    "    print(top_display.to_string(index=False))\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Highlight winner\n",
    "    winner = top_experiments.iloc[0]\n",
    "    print(f\"\\nüèÜ Best Configuration:\")\n",
    "    print(f\"   ID: {winner['id']}\")\n",
    "    print(f\"   Name: {winner['name']}\")\n",
    "    if 'probe_type' in winner:\n",
    "        print(f\"   Probe: {winner['probe_type']}\")\n",
    "    if 'model_type' in winner:\n",
    "        print(f\"   Model: {winner['model_type']}\")\n",
    "    print(f\"   Top-1 Accuracy: {winner['top_1_accuracy']:.4f}\")\n",
    "    print(f\"   Training Time: {winner['training_time_seconds']:.1f}s\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for top experiments comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Comparison Plots\n",
    "\n",
    "Create comprehensive visualizations comparing different aspects of experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Probe Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_df) > 0 and 'probe_type' in filtered_df.columns and 'top_1_accuracy' in filtered_df.columns:\n",
    "    # Box plot for probe comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    probe_data = []\n",
    "    probe_labels = []\n",
    "    for probe in filtered_df['probe_type'].unique():\n",
    "        probe_exps = filtered_df[filtered_df['probe_type'] == probe]\n",
    "        probe_data.append(probe_exps['top_1_accuracy'].values)\n",
    "        probe_labels.append(probe)\n",
    "    \n",
    "    bp = axes[0].boxplot(probe_data, labels=probe_labels, patch_artist=True)\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set_facecolor('lightblue')\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    axes[0].set_xlabel('Probe Type', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Top-1 Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Accuracy Distribution by Probe Type', fontsize=13, fontweight='bold')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[0].set_ylim([0, 1])\n",
    "    \n",
    "    # Training time comparison\n",
    "    time_data = []\n",
    "    for probe in probe_labels:\n",
    "        probe_exps = filtered_df[filtered_df['probe_type'] == probe]\n",
    "        time_data.append(probe_exps['training_time_seconds'].values)\n",
    "    \n",
    "    bp2 = axes[1].boxplot(time_data, labels=probe_labels, patch_artist=True)\n",
    "    for patch in bp2['boxes']:\n",
    "        patch.set_facecolor('lightcoral')\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    axes[1].set_xlabel('Probe Type', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Training Time by Probe Type', fontsize=13, fontweight='bold')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/probe_comparison_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Probe comparison plots saved to: outputs/probe_comparison_detailed.png\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for probe comparison plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_df) > 0 and 'model_type' in filtered_df.columns and 'top_1_accuracy' in filtered_df.columns:\n",
    "    # Group by model type\n",
    "    model_stats = filtered_df.groupby('model_type').agg({\n",
    "        'top_1_accuracy': ['mean', 'std', 'count'],\n",
    "        'training_time_seconds': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    model_stats.columns = ['model_type', 'acc_mean', 'acc_std', 'count', 'time_mean', 'time_std']\n",
    "    \n",
    "    # Bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x_pos = np.arange(len(model_stats))\n",
    "    ax.bar(x_pos, model_stats['acc_mean'], yerr=model_stats['acc_std'],\n",
    "          capsize=8, alpha=0.8, color='teal', edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xlabel('Model Type', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Top-1 Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Model Architecture Comparison (Mean ¬± Std Dev)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(model_stats['model_type'], rotation=0)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (mean, std, count) in enumerate(zip(model_stats['acc_mean'], \n",
    "                                                model_stats['acc_std'],\n",
    "                                                model_stats['count'])):\n",
    "        ax.text(i, mean + std + 0.02, f\"{mean:.3f}\\n(n={int(count)})\", \n",
    "               ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Model comparison plot saved to: outputs/model_comparison.png\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for model comparison plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Accuracy vs Training Time Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_df) > 0 and 'top_1_accuracy' in filtered_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Scatter plot colored by probe type\n",
    "    if 'probe_type' in filtered_df.columns:\n",
    "        for probe in filtered_df['probe_type'].unique():\n",
    "            probe_data = filtered_df[filtered_df['probe_type'] == probe]\n",
    "            ax.scatter(probe_data['training_time_seconds'], \n",
    "                      probe_data['top_1_accuracy'],\n",
    "                      s=100, alpha=0.6, label=probe, edgecolors='black', linewidth=1)\n",
    "    else:\n",
    "        ax.scatter(filtered_df['training_time_seconds'], \n",
    "                  filtered_df['top_1_accuracy'],\n",
    "                  s=100, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Training Time (seconds)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Top-1 Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Pareto Front: Accuracy vs Training Time', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    if 'probe_type' in filtered_df.columns:\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "    \n",
    "    # Highlight Pareto optimal points\n",
    "    # Find points that are not dominated (higher accuracy AND lower time)\n",
    "    pareto_points = []\n",
    "    for idx, row in filtered_df.iterrows():\n",
    "        is_pareto = True\n",
    "        for idx2, row2 in filtered_df.iterrows():\n",
    "            if (row2['top_1_accuracy'] >= row['top_1_accuracy'] and \n",
    "                row2['training_time_seconds'] <= row['training_time_seconds'] and\n",
    "                (row2['top_1_accuracy'] > row['top_1_accuracy'] or \n",
    "                 row2['training_time_seconds'] < row['training_time_seconds'])):\n",
    "                is_pareto = False\n",
    "                break\n",
    "        if is_pareto:\n",
    "            pareto_points.append((row['training_time_seconds'], row['top_1_accuracy']))\n",
    "    \n",
    "    if pareto_points:\n",
    "        pareto_points = sorted(pareto_points, key=lambda x: x[0])\n",
    "        pareto_x, pareto_y = zip(*pareto_points)\n",
    "        ax.plot(pareto_x, pareto_y, 'r--', linewidth=2, alpha=0.5, label='Pareto Front')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/pareto_front.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Pareto front plot saved to: outputs/pareto_front.png\")\n",
    "    print(f\"   Found {len(pareto_points)} Pareto-optimal configurations\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for Pareto front analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Best Configuration\n",
    "\n",
    "Save the best-performing configuration for deployment or further experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_df) > 0 and 'top_1_accuracy' in filtered_df.columns:\n",
    "    # Get best experiment\n",
    "    best_exp = filtered_df.nlargest(1, 'top_1_accuracy').iloc[0]\n",
    "    best_id = best_exp['id']\n",
    "    \n",
    "    # Load full details from database\n",
    "    conn = sqlite3.connect(\"results.db\")\n",
    "    query = f\"SELECT config FROM experiments WHERE id = {best_id}\"\n",
    "    result = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    if len(result) > 0:\n",
    "        import json\n",
    "        best_config = json.loads(result['config'].iloc[0])\n",
    "        \n",
    "        # Save to file\n",
    "        output_path = Path('outputs/best_configuration.json')\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(best_config, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"BEST CONFIGURATION EXPORTED\")\n",
    "        print(\"=\"*90)\n",
    "        print(f\"\\nSaved to: {output_path}\")\n",
    "        print(f\"\\nConfiguration Summary:\")\n",
    "        print(f\"  Probe: {best_config.get('probe_type', 'N/A')}\")\n",
    "        print(f\"  Model: {best_config.get('model_type', 'N/A')}\")\n",
    "        print(f\"  Top-1 Accuracy: {best_exp['top_1_accuracy']:.4f}\")\n",
    "        print(f\"  Training Time: {best_exp['training_time_seconds']:.1f}s\")\n",
    "        \n",
    "        if 'system' in best_config:\n",
    "            print(f\"\\n  System Parameters:\")\n",
    "            for key, value in best_config['system'].items():\n",
    "                print(f\"    {key}: {value}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Could not load best configuration from database.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No experiments available for export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Statistical Significance Tests (Optional)\n",
    "\n",
    "Perform pairwise statistical tests to determine if performance differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCIPY_AVAILABLE and len(filtered_df) > 0 and 'probe_type' in filtered_df.columns:\n",
    "    # Get unique probes with at least 3 samples each\n",
    "    probe_counts = filtered_df['probe_type'].value_counts()\n",
    "    valid_probes = probe_counts[probe_counts >= 3].index.tolist()\n",
    "    \n",
    "    if len(valid_probes) >= 2:\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"PAIRWISE STATISTICAL TESTS (T-Tests)\")\n",
    "        print(\"=\"*90)\n",
    "        print(\"\\nComparing top-1 accuracy between probe types...\\n\")\n",
    "        \n",
    "        # Perform pairwise t-tests\n",
    "        results = []\n",
    "        for i, probe1 in enumerate(valid_probes):\n",
    "            for probe2 in valid_probes[i+1:]:\n",
    "                data1 = filtered_df[filtered_df['probe_type'] == probe1]['top_1_accuracy'].values\n",
    "                data2 = filtered_df[filtered_df['probe_type'] == probe2]['top_1_accuracy'].values\n",
    "                \n",
    "                # Two-sample t-test\n",
    "                t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "                \n",
    "                # Effect size (Cohen's d)\n",
    "                pooled_std = np.sqrt((np.var(data1) + np.var(data2)) / 2)\n",
    "                cohens_d = (np.mean(data1) - np.mean(data2)) / pooled_std if pooled_std > 0 else 0\n",
    "                \n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                \n",
    "                results.append({\n",
    "                    'Probe 1': probe1,\n",
    "                    'Probe 2': probe2,\n",
    "                    'Mean Diff': f\"{np.mean(data1) - np.mean(data2):.4f}\",\n",
    "                    'p-value': f\"{p_value:.4f}\",\n",
    "                    'Significance': significance,\n",
    "                    \"Cohen's d\": f\"{cohens_d:.3f}\"\n",
    "                })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(results_df.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(\"Significance levels: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "        print(\"Cohen's d effect size: |d|<0.2 (small), |d|<0.5 (medium), |d|>=0.5 (large)\")\n",
    "        print(\"=\"*90)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Insufficient data for statistical tests (need at least 3 samples per probe type).\")\n",
    "elif not SCIPY_AVAILABLE:\n",
    "    print(\"\\n‚ö†Ô∏è  scipy not available - install with: pip install scipy\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for statistical tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fidelity Gap Analysis (Optional)\n",
    "\n",
    "If you've run both low-fidelity and high-fidelity experiments, analyze how well low-fidelity results predict high-fidelity performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cross-fidelity data exists\n",
    "# This requires experiments with 'fidelity' tag in their names or metadata\n",
    "\n",
    "low_fidelity = filtered_df[filtered_df['name'].str.contains('low_fidelity|quick_test', case=False, na=False)]\n",
    "high_fidelity = filtered_df[filtered_df['name'].str.contains('high_fidelity|full', case=False, na=False)]\n",
    "\n",
    "if len(low_fidelity) > 0 and len(high_fidelity) > 0:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"FIDELITY GAP ANALYSIS\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"\\nLow-Fidelity Experiments: {len(low_fidelity)}\")\n",
    "    print(f\"High-Fidelity Experiments: {len(high_fidelity)}\")\n",
    "    \n",
    "    # Compare statistics\n",
    "    if 'top_1_accuracy' in low_fidelity.columns and 'top_1_accuracy' in high_fidelity.columns:\n",
    "        low_mean = low_fidelity['top_1_accuracy'].mean()\n",
    "        high_mean = high_fidelity['top_1_accuracy'].mean()\n",
    "        gap = high_mean - low_mean\n",
    "        \n",
    "        print(f\"\\nMean Low-Fidelity Accuracy: {low_mean:.4f}\")\n",
    "        print(f\"Mean High-Fidelity Accuracy: {high_mean:.4f}\")\n",
    "        print(f\"Fidelity Gap: {gap:.4f} ({gap/high_mean*100:.2f}%)\")\n",
    "        \n",
    "        if gap > 0:\n",
    "            print(\"\\n‚úì Low-fidelity experiments underestimate performance (conservative)\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Low-fidelity experiments overestimate performance\")\n",
    "        \n",
    "        # Correlation analysis\n",
    "        if SCIPY_AVAILABLE and len(low_fidelity) == len(high_fidelity):\n",
    "            correlation, p_value = stats.pearsonr(\n",
    "                low_fidelity['top_1_accuracy'].values[:min(len(low_fidelity), len(high_fidelity))],\n",
    "                high_fidelity['top_1_accuracy'].values[:min(len(low_fidelity), len(high_fidelity))]\n",
    "            )\n",
    "            print(f\"\\nRanking Correlation: r={correlation:.4f}, p={p_value:.4f}\")\n",
    "            if correlation > 0.8:\n",
    "                print(\"‚úì Strong correlation - low-fidelity is a good predictor\")\n",
    "            elif correlation > 0.5:\n",
    "                print(\"‚ö†Ô∏è  Moderate correlation - use with caution\")\n",
    "            else:\n",
    "                print(\"‚úó Weak correlation - low-fidelity is not reliable\")\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No cross-fidelity data found for analysis.\")\n",
    "    print(\"   Run experiments with 'low_fidelity' and 'high_fidelity' tags to enable this analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Recommendations\n",
    "\n",
    "Generate actionable recommendations based on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_df) > 0 and 'top_1_accuracy' in filtered_df.columns:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"FINAL RECOMMENDATIONS\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Best probe\n",
    "    if 'probe_type' in filtered_df.columns:\n",
    "        probe_means = filtered_df.groupby('probe_type')['top_1_accuracy'].mean().sort_values(ascending=False)\n",
    "        best_probe = probe_means.index[0]\n",
    "        print(f\"\\n1. **Best Probe Design**: {best_probe}\")\n",
    "        print(f\"   - Mean Accuracy: {probe_means.iloc[0]:.4f}\")\n",
    "        print(f\"   - Advantage over baseline: {(probe_means.iloc[0] - probe_means.iloc[-1])*100:.2f}%\")\n",
    "    \n",
    "    # Best model\n",
    "    if 'model_type' in filtered_df.columns:\n",
    "        model_means = filtered_df.groupby('model_type')['top_1_accuracy'].mean().sort_values(ascending=False)\n",
    "        best_model = model_means.index[0]\n",
    "        print(f\"\\n2. **Best Model Architecture**: {best_model}\")\n",
    "        print(f\"   - Mean Accuracy: {model_means.iloc[0]:.4f}\")\n",
    "    \n",
    "    # Training efficiency\n",
    "    efficiency = filtered_df['top_1_accuracy'] / (filtered_df['training_time_seconds'] / 60)  # Acc per minute\n",
    "    most_efficient_idx = efficiency.idxmax()\n",
    "    most_efficient = filtered_df.loc[most_efficient_idx]\n",
    "    print(f\"\\n3. **Most Efficient Configuration**: {most_efficient['name']}\")\n",
    "    if 'probe_type' in most_efficient:\n",
    "        print(f\"   - Probe: {most_efficient['probe_type']}\")\n",
    "    if 'model_type' in most_efficient:\n",
    "        print(f\"   - Model: {most_efficient['model_type']}\")\n",
    "    print(f\"   - Efficiency: {efficiency.loc[most_efficient_idx]:.4f} acc/min\")\n",
    "    \n",
    "    # Performance variability\n",
    "    if 'probe_type' in filtered_df.columns:\n",
    "        probe_stds = filtered_df.groupby('probe_type')['top_1_accuracy'].std().sort_values()\n",
    "        most_stable = probe_stds.index[0]\n",
    "        print(f\"\\n4. **Most Stable Probe** (lowest variance): {most_stable}\")\n",
    "        print(f\"   - Std Dev: {probe_stds.iloc[0]:.4f}\")\n",
    "        print(f\"   - Good choice for production deployment\")\n",
    "    \n",
    "    # Overall winner\n",
    "    best_overall = filtered_df.nlargest(1, 'top_1_accuracy').iloc[0]\n",
    "    print(f\"\\n5. **Overall Winner** (highest accuracy): {best_overall['name']}\")\n",
    "    if 'probe_type' in best_overall:\n",
    "        print(f\"   - Probe: {best_overall['probe_type']}\")\n",
    "    if 'model_type' in best_overall:\n",
    "        print(f\"   - Model: {best_overall['model_type']}\")\n",
    "    print(f\"   - Top-1 Accuracy: {best_overall['top_1_accuracy']:.4f}\")\n",
    "    print(f\"   - Recommended for maximum performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"\\nüìä Analysis complete! Results saved to outputs/ directory.\")\n",
    "    print(\"üìÑ Best configuration exported to: outputs/best_configuration.json\")\n",
    "    print(\"üìà Plots available in: outputs/\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"  - Deploy best configuration for production\")\n",
    "    print(\"  - Run additional seeds to confirm results\")\n",
    "    print(\"  - Test on real-world data if using synthetic\")\n",
    "    print(\"  - Consider ensemble methods combining top performers\")\n",
    "    print(\"=\"*90)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No data available for generating recommendations.\")\n",
    "    print(\"   Run some experiments first using 01_quickstart.ipynb or 03_run_search.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided comprehensive analysis tools for your RIS experiments:\n",
    "\n",
    "‚úì **Database Querying** - Loaded and filtered experiment results\n",
    "\n",
    "‚úì **Statistical Analysis** - Computed means, standard deviations, and confidence intervals\n",
    "\n",
    "‚úì **Comparison Plots** - Visualized probe, model, and Pareto front comparisons\n",
    "\n",
    "‚úì **Best Configuration Export** - Saved optimal settings for deployment\n",
    "\n",
    "‚úì **Significance Testing** - Validated performance differences (if scipy available)\n",
    "\n",
    "‚úì **Recommendations** - Generated actionable insights\n",
    "\n",
    "### Key Outputs:\n",
    "- `outputs/probe_comparison_detailed.png` - Probe performance boxplots\n",
    "- `outputs/model_comparison.png` - Model architecture comparison\n",
    "- `outputs/pareto_front.png` - Accuracy vs time trade-offs\n",
    "- `outputs/best_configuration.json` - Best performing configuration\n",
    "\n",
    "### Further Analysis:\n",
    "For more advanced analyses, you can:\n",
    "- Query the SQLite database directly for custom analyses\n",
    "- Use the `ResultAnalyzer` class for programmatic access\n",
    "- Generate LaTeX tables with `ReportGenerator`\n",
    "- Perform cross-validation studies\n",
    "- Analyze hyperparameter sensitivity\n",
    "\n",
    "### Questions?\n",
    "- Check `docs/analysis_guide.md` for detailed documentation\n",
    "- See `examples/advanced_analysis.py` for code examples\n",
    "- Open an issue on GitHub for support\n",
    "\n",
    "Thank you for using the RIS Auto-Research Engine! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
