{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis\n",
    "\n",
    "Load past experiments, compare configurations, generate reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ris_research_engine.engine import ResultAnalyzer\n",
    "from ris_research_engine.foundation import ResultTracker\n",
    "\n",
    "tracker = ResultTracker(\"outputs/experiments/results.db\")\n",
    "analyzer = ResultAnalyzer(tracker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = tracker.get_all_experiments()\n",
    "print(f\"Total experiments: {len(experiments)}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if experiments:\n",
    "    df = pd.DataFrame([{\n",
    "        'id': e['id'],\n",
    "        'name': e['name'][:40],\n",
    "        'probe': e['probe_type'],\n",
    "        'model': e['model_type'],\n",
    "        'M': e['M'],\n",
    "        'K': e['K'],\n",
    "        'accuracy': e['metrics'].get('top_1_accuracy', 0),\n",
    "        'power_ratio': e['metrics'].get('power_ratio', 0),\n",
    "        'status': e['status'],\n",
    "        'time': f\"{e['training_time_seconds']:.1f}s\"\n",
    "    } for e in experiments])\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No experiments found. Run some experiments first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_comparison = analyzer.compare_probes()\n",
    "\n",
    "if not probe_comparison.empty:\n",
    "    print(\"Probe Performance Comparison:\")\n",
    "    print(\"=\"*60)\n",
    "    display(probe_comparison)\n",
    "    \n",
    "    # Plot\n",
    "    fig = analyzer.plot_probe_comparison()\n",
    "    if fig:\n",
    "        display(fig)\n",
    "else:\n",
    "    print(\"Not enough data for probe comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = analyzer.compare_models()\n",
    "\n",
    "if not model_comparison.empty:\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(\"=\"*60)\n",
    "    display(model_comparison)\n",
    "    \n",
    "    # Plot\n",
    "    fig = analyzer.plot_model_comparison()\n",
    "    if fig:\n",
    "        display(fig)\n",
    "else:\n",
    "    print(\"Not enough data for model comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_analysis = analyzer.sparsity_analysis()\n",
    "\n",
    "if not sparsity_analysis.empty:\n",
    "    print(\"Sparsity Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    display(sparsity_analysis.head(10))\n",
    "    \n",
    "    # Plot accuracy vs M/K\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for probe in sparsity_analysis['probe_type'].unique():\n",
    "        data = sparsity_analysis[sparsity_analysis['probe_type'] == probe]\n",
    "        plt.plot(data['M']/data['K'], data['top_1_accuracy'], marker='o', label=probe)\n",
    "    \n",
    "    plt.xlabel('Sensing Budget (M/K)')\n",
    "    plt.ylabel('Top-1 Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Sparsity')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for sparsity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fidelity Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_analysis = analyzer.fidelity_gap_analysis()\n",
    "\n",
    "if not gap_analysis.empty:\n",
    "    print(\"Cross-Fidelity Gap Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    display(gap_analysis)\n",
    "    \n",
    "    # Plot\n",
    "    fig = analyzer.plot_fidelity_gap()\n",
    "    if fig:\n",
    "        display(fig)\n",
    "else:\n",
    "    print(\"No cross-fidelity experiments found\")\n",
    "    print(\"Run experiments with different data_fidelity values to enable this analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = analyzer.best_configuration(metric='top_1_accuracy')\n",
    "\n",
    "if best:\n",
    "    print(\"Best Configuration:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Name: {best.config.name}\")\n",
    "    print(f\"Probe: {best.config.probe_type}\")\n",
    "    print(f\"Model: {best.config.model_type}\")\n",
    "    print(f\"M/K: {best.config.system.M}/{best.config.system.K}\")\n",
    "    print(f\"Accuracy: {best.metrics['top_1_accuracy']:.3f}\")\n",
    "    print(f\"Training time: {best.training_time_seconds:.1f}s\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    import json\n",
    "    output_file = 'outputs/best_config.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(best.config.to_dict(), f, indent=2)\n",
    "    print(f\"\\n\u2705 Saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No completed experiments found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best experiment's training history\n",
    "if best:\n",
    "    # Find the experiment ID from the database\n",
    "    best_exp = tracker.get_best_experiment(metric_name='top_1_accuracy')\n",
    "    if best_exp:\n",
    "        fig = analyzer.plot_training_curves(best_exp['id'])\n",
    "        if fig:\n",
    "            display(fig)\n",
    "else:\n",
    "    print(\"No experiments to display\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}