{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Search Campaigns\n",
    "\n",
    "Run systematic searches across probe types, models, and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ris_research_engine.ui import RISEngine\n",
    "\n",
    "engine = RISEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Quick Test Campaign\n",
    "\n",
    "Execute a minimal search campaign for testing (~2-5 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Search campaigns require proper integration with search strategies\n",
    "# For now, we'll use the simple run() method\n",
    "\n",
    "print(\"Running quick test experiments...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test 1: Random uniform probe\n",
    "result1 = engine.run(\n",
    "    probe=\"random_uniform\",\n",
    "    model=\"mlp\",\n",
    "    M=4,\n",
    "    K=16,\n",
    "    N=16,\n",
    "    data=\"synthetic_rayleigh\",\n",
    "    n_samples=500,\n",
    "    epochs=10,\n",
    "    tags=[\"quick_test_campaign\"]\n",
    ")\n",
    "results.append(result1)\n",
    "\n",
    "# Test 2: Hadamard probe\n",
    "result2 = engine.run(\n",
    "    probe=\"hadamard\",\n",
    "    model=\"mlp\",\n",
    "    M=4,\n",
    "    K=16,\n",
    "    N=16,\n",
    "    data=\"synthetic_rayleigh\",\n",
    "    n_samples=500,\n",
    "    epochs=10,\n",
    "    tags=[\"quick_test_campaign\"]\n",
    ")\n",
    "results.append(result2)\n",
    "\n",
    "print(f\"\\n\u2705 Completed {len(results)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "Show metrics for all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for result in results:\n",
    "    comparison_data.append({\n",
    "        'Probe': result.config.probe_type,\n",
    "        'Model': result.config.model_type,\n",
    "        'Top-1 Accuracy': result.metrics.get('top_1_accuracy', 0.0),\n",
    "        'Power Ratio': result.metrics.get('power_ratio', 0.0),\n",
    "        'Training Time (s)': result.training_time_seconds,\n",
    "        'Status': result.status\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nCampaign Results:\")\n",
    "print(\"=\"*60)\n",
    "display(df)\n",
    "\n",
    "# Find best result\n",
    "best_idx = df['Top-1 Accuracy'].idxmax()\n",
    "print(f\"\\n\ud83c\udfc6 Best configuration:\")\n",
    "print(f\"   Probe: {df.loc[best_idx, 'Probe']}\")\n",
    "print(f\"   Accuracy: {df.loc[best_idx, 'Top-1 Accuracy']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.plot_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Probe Comparison (Optional)\n",
    "\n",
    "\u26a0\ufe0f This will take approximately 20-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run full probe comparison\n",
    "# results = engine.compare_probes(\n",
    "#     probes=[\"random_uniform\", \"hadamard\", \"sobol\", \"dft_beams\"],\n",
    "#     model=\"mlp\",\n",
    "#     M=8,\n",
    "#     K=64,\n",
    "#     N=64,\n",
    "#     n_samples=10000,\n",
    "#     epochs=50\n",
    "# )\n",
    "# engine.plot_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Fidelity Validation (Optional)\n",
    "\n",
    "Validate synthetic winners on Sionna data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and update path to run cross-fidelity validation\n",
    "# gap_report = engine.validate_on_sionna(\n",
    "#     campaign_name=\"quick_test_campaign\",\n",
    "#     hdf5_path=\"/path/to/sionna_data.h5\",  # Update with actual path\n",
    "#     top_n=2\n",
    "# )\n",
    "# engine.plot_fidelity_gap(gap_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}