{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIS Auto-Research Engine - Run Search Campaigns\n",
    "\n",
    "This notebook demonstrates how to execute comprehensive search campaigns using predefined YAML configurations. You'll learn to:\n",
    "- List and select available search space configurations\n",
    "- Execute multi-experiment campaigns\n",
    "- Monitor progress and collect results\n",
    "- Generate publication-ready visualizations\n",
    "- Export data for further analysis\n",
    "\n",
    "**Available Campaigns:**\n",
    "- `quick_test` - Fast validation (2 experiments, ~2 min)\n",
    "- `probe_comparison` - Compare 6 probe types (18 experiments, ~15 min)\n",
    "- `model_comparison` - Compare 3 architectures (9 experiments, ~10 min)\n",
    "- `sparsity_sweep` - Parameter optimization (45 experiments, ~30 min)\n",
    "- `cross_fidelity_validation` - Multi-fidelity analysis (30 experiments, ~25 min)\n",
    "- `full_search` - Comprehensive search (200+ experiments, several hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization\n",
    "\n",
    "Import necessary modules and initialize the RIS Engine with result tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "# RIS Engine imports\n",
    "from ris_research_engine.ui import RISEngine\n",
    "from ris_research_engine.engine import ResultAnalyzer, ReportGenerator\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available else 'default')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Initialize engine\n",
    "engine = RISEngine(\n",
    "    db_path=\"results.db\",\n",
    "    output_dir=\"outputs\"\n",
    ")\n",
    "\n",
    "print(\"‚úì RIS Engine initialized\")\n",
    "print(f\"  Database: {engine.db_path}\")\n",
    "print(f\"  Output directory: {engine.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Available Configurations\n",
    "\n",
    "Discover all available search space configurations with descriptions and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to search space configs\n",
    "config_dir = Path(\"../configs/search_spaces\")\n",
    "\n",
    "# List all YAML configs\n",
    "configs = list(config_dir.glob(\"*.yaml\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AVAILABLE SEARCH SPACE CONFIGURATIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "config_info = []\n",
    "for config_path in sorted(configs):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config_data = yaml.safe_load(f)\n",
    "    \n",
    "    # Extract key information\n",
    "    name = config_data.get('name', 'N/A')\n",
    "    description = config_data.get('description', 'No description')\n",
    "    \n",
    "    # Count expected experiments (rough estimate)\n",
    "    # This is a simplified calculation\n",
    "    num_probes = 1\n",
    "    if 'probes' in config_data and 'type' in config_data['probes']:\n",
    "        probe_types = config_data['probes']['type']\n",
    "        if isinstance(probe_types, list):\n",
    "            num_probes = len(probe_types)\n",
    "    \n",
    "    num_seeds = len(config_data.get('random_seeds', [1]))\n",
    "    estimated_experiments = num_probes * num_seeds\n",
    "    \n",
    "    config_info.append({\n",
    "        'Config File': config_path.name,\n",
    "        'Name': name,\n",
    "        'Description': description[:60] + '...' if len(description) > 60 else description,\n",
    "        'Est. Experiments': estimated_experiments\n",
    "    })\n",
    "    \n",
    "    print(f\"üìã {config_path.name}\")\n",
    "    print(f\"   Name: {name}\")\n",
    "    print(f\"   Description: {description}\")\n",
    "    print(f\"   Estimated Experiments: ~{estimated_experiments}\")\n",
    "    print()\n",
    "\n",
    "# Display as table\n",
    "config_df = pd.DataFrame(config_info)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(config_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Probe Comparison Campaign\n",
    "\n",
    "Execute the `probe_comparison.yaml` campaign which systematically compares 6 different probe designs:\n",
    "- DFT beams (structured)\n",
    "- Hadamard (orthogonal)\n",
    "- Sobol (quasi-random)\n",
    "- Halton (quasi-random)\n",
    "- Random uniform (baseline)\n",
    "- Random binary (baseline)\n",
    "\n",
    "This campaign helps identify the most effective probe design for your system configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run probe comparison campaign\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING PROBE COMPARISON CAMPAIGN\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis will run ~18 experiments (6 probes √ó 3 seeds)\")\n",
    "print(\"Expected runtime: ~15 minutes on CPU\\n\")\n",
    "\n",
    "try:\n",
    "    # Run the campaign\n",
    "    probe_campaign = engine.search(\"../configs/search_spaces/probe_comparison.yaml\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì PROBE COMPARISON CAMPAIGN COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"  Total Experiments: {len(probe_campaign['experiments'])}\")\n",
    "    print(f\"  Successful: {sum(1 for e in probe_campaign['experiments'] if e['status'] == 'completed')}\")\n",
    "    print(f\"  Failed: {sum(1 for e in probe_campaign['experiments'] if e['status'] == 'failed')}\")\n",
    "    print(f\"  Total Runtime: {probe_campaign.get('total_time_seconds', 0):.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Campaign failed: {e}\")\n",
    "    print(\"  Check logs for details\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Campaign Summary Statistics\n",
    "\n",
    "Display summary statistics for the completed campaign including mean, std, min, and max values for each probe type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze campaign results\n",
    "experiments = probe_campaign['experiments']\n",
    "completed = [e for e in experiments if e['status'] == 'completed']\n",
    "\n",
    "# Group by probe type\n",
    "probe_results = {}\n",
    "for exp in completed:\n",
    "    probe = exp['config']['probe_type']\n",
    "    if probe not in probe_results:\n",
    "        probe_results[probe] = []\n",
    "    probe_results[probe].append(exp['metrics']['top_1_accuracy'])\n",
    "\n",
    "# Calculate statistics\n",
    "summary_data = []\n",
    "for probe, accuracies in probe_results.items():\n",
    "    summary_data.append({\n",
    "        'Probe Type': probe,\n",
    "        'Mean Accuracy': np.mean(accuracies),\n",
    "        'Std Dev': np.std(accuracies),\n",
    "        'Min': np.min(accuracies),\n",
    "        'Max': np.max(accuracies),\n",
    "        'Runs': len(accuracies)\n",
    "    })\n",
    "\n",
    "# Sort by mean accuracy\n",
    "summary_df = pd.DataFrame(summary_data).sort_values('Mean Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROBE COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Format display\n",
    "for col in ['Mean Accuracy', 'Std Dev', 'Min', 'Max']:\n",
    "    summary_df[col] = summary_df[col].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify winner\n",
    "best_probe = summary_data[0]['Probe Type']\n",
    "best_acc = summary_data[0]['Mean Accuracy']\n",
    "print(f\"\\nüèÜ Best Probe: {best_probe}\")\n",
    "print(f\"   Mean Top-1 Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Probe Comparison Plots\n",
    "\n",
    "Create publication-quality visualizations comparing probe performance with error bars showing variance across random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart with error bars\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "probes = [d['Probe Type'] for d in summary_data]\n",
    "means = [d['Mean Accuracy'] for d in summary_data]\n",
    "stds = [d['Std Dev'] for d in summary_data]\n",
    "\n",
    "# Create bar chart\n",
    "x_pos = np.arange(len(probes))\n",
    "bars = ax.bar(x_pos, means, yerr=stds, capsize=8, \n",
    "              alpha=0.8, color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Highlight best probe\n",
    "bars[0].set_color('darkgreen')\n",
    "bars[0].set_alpha(0.9)\n",
    "\n",
    "ax.set_xlabel('Probe Type', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Top-1 Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Probe Design Comparison (Mean ¬± Std Dev over 3 seeds)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(probes, rotation=45, ha='right')\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, mean + std + 0.02, f\"{mean:.3f}\", \n",
    "           ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(engine.output_dir / 'probe_comparison_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Plot saved to: outputs/probe_comparison_bar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves Comparison\n",
    "\n",
    "Visualize convergence behavior by plotting training curves for all probe types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for each probe\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Group experiments by probe\n",
    "for idx, (probe, _) in enumerate(probe_results.items()):\n",
    "    if idx >= 6:\n",
    "        break\n",
    "        \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get all experiments for this probe\n",
    "    probe_experiments = [e for e in completed if e['config']['probe_type'] == probe]\n",
    "    \n",
    "    # Plot each seed\n",
    "    for exp in probe_experiments:\n",
    "        if 'training_history' in exp:\n",
    "            history = exp['training_history']\n",
    "            if 'val_top_1_accuracy' in history:\n",
    "                epochs = range(1, len(history['val_top_1_accuracy']) + 1)\n",
    "                ax.plot(epochs, history['val_top_1_accuracy'], \n",
    "                       alpha=0.6, linewidth=2)\n",
    "    \n",
    "    # Calculate mean curve\n",
    "    all_curves = []\n",
    "    for exp in probe_experiments:\n",
    "        if 'training_history' in exp and 'val_top_1_accuracy' in exp['training_history']:\n",
    "            all_curves.append(exp['training_history']['val_top_1_accuracy'])\n",
    "    \n",
    "    if all_curves:\n",
    "        # Find minimum length\n",
    "        min_len = min(len(c) for c in all_curves)\n",
    "        truncated = [c[:min_len] for c in all_curves]\n",
    "        mean_curve = np.mean(truncated, axis=0)\n",
    "        epochs = range(1, len(mean_curve) + 1)\n",
    "        ax.plot(epochs, mean_curve, 'r-', linewidth=3, label='Mean')\n",
    "    \n",
    "    ax.set_xlabel('Epoch', fontsize=10)\n",
    "    ax.set_ylabel('Validation Accuracy', fontsize=10)\n",
    "    ax.set_title(f'{probe}', fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "    if all_curves:\n",
    "        ax.legend(loc='lower right')\n",
    "\n",
    "fig.suptitle('Training Curves by Probe Type', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(engine.output_dir / 'probe_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Plot saved to: outputs/probe_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional: Run Sparsity Sweep Campaign\n",
    "\n",
    "The sparsity sweep explores how varying the measurement budget (M) affects performance. This campaign runs 45 experiments testing M values from 4 to 32.\n",
    "\n",
    "**Warning:** This campaign takes ~30 minutes. Comment out this cell if you want to skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run sparsity sweep\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"STARTING SPARSITY SWEEP CAMPAIGN\")\n",
    "# print(\"=\"*80)\n",
    "# print(\"\\nThis will run ~45 experiments (5 M values √ó 3 probes √ó 3 seeds)\")\n",
    "# print(\"Expected runtime: ~30 minutes on CPU\\n\")\n",
    "#\n",
    "# try:\n",
    "#     sparsity_campaign = engine.search(\"../configs/search_spaces/sparsity_sweep.yaml\")\n",
    "#     print(\"\\n‚úì Sparsity sweep campaign completed!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\n‚úó Campaign failed: {e}\")\n",
    "\n",
    "print(\"\\n‚ÑπÔ∏è  Sparsity sweep is commented out by default.\")\n",
    "print(\"   Uncomment the code above to run this campaign.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sparsity Analysis Plots\n",
    "\n",
    "If you ran the sparsity sweep, visualize how accuracy varies with measurement budget M. This shows the trade-off between sensing overhead and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if sparsity campaign was run\n",
    "if 'sparsity_campaign' in locals():\n",
    "    # Analyze sparsity results\n",
    "    sparsity_experiments = sparsity_campaign['experiments']\n",
    "    sparsity_completed = [e for e in sparsity_experiments if e['status'] == 'completed']\n",
    "    \n",
    "    # Group by M value and probe\n",
    "    sparsity_data = {}\n",
    "    for exp in sparsity_completed:\n",
    "        M = exp['config']['system']['M']\n",
    "        probe = exp['config']['probe_type']\n",
    "        key = (M, probe)\n",
    "        \n",
    "        if key not in sparsity_data:\n",
    "            sparsity_data[key] = []\n",
    "        sparsity_data[key].append(exp['metrics']['top_1_accuracy'])\n",
    "    \n",
    "    # Plot M vs accuracy for each probe\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Get unique probes\n",
    "    probes = sorted(set(key[1] for key in sparsity_data.keys()))\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'p']\n",
    "    \n",
    "    for idx, probe in enumerate(probes):\n",
    "        # Get data for this probe\n",
    "        M_values = []\n",
    "        mean_accs = []\n",
    "        std_accs = []\n",
    "        \n",
    "        for M in sorted(set(key[0] for key in sparsity_data.keys())):\n",
    "            if (M, probe) in sparsity_data:\n",
    "                accs = sparsity_data[(M, probe)]\n",
    "                M_values.append(M)\n",
    "                mean_accs.append(np.mean(accs))\n",
    "                std_accs.append(np.std(accs))\n",
    "        \n",
    "        # Plot with error bars\n",
    "        ax.errorbar(M_values, mean_accs, yerr=std_accs, \n",
    "                   marker=markers[idx % len(markers)], markersize=8,\n",
    "                   linewidth=2.5, capsize=5, capthick=2,\n",
    "                   label=probe, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Measurement Budget (M)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Top-1 Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Sparsity Analysis: Accuracy vs. Measurement Budget', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(engine.output_dir / 'sparsity_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Plot saved to: outputs/sparsity_analysis.png\")\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è  Sparsity campaign not run. Skipping analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results to CSV\n",
    "\n",
    "Export all experiment results to CSV format for further analysis in Excel, R, or other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export probe comparison results\n",
    "export_data = []\n",
    "for exp in completed:\n",
    "    export_data.append({\n",
    "        'experiment_id': exp.get('experiment_id', 'N/A'),\n",
    "        'name': exp['config']['name'],\n",
    "        'probe_type': exp['config']['probe_type'],\n",
    "        'model_type': exp['config']['model_type'],\n",
    "        'N': exp['config']['system']['N'],\n",
    "        'K': exp['config']['system']['K'],\n",
    "        'M': exp['config']['system']['M'],\n",
    "        'top_1_accuracy': exp['metrics']['top_1_accuracy'],\n",
    "        'top_5_accuracy': exp['metrics'].get('top_5_accuracy', 0),\n",
    "        'val_loss': exp['metrics'].get('val_loss', 0),\n",
    "        'training_time_seconds': exp['training_time_seconds'],\n",
    "        'total_epochs': exp['total_epochs'],\n",
    "        'best_epoch': exp['best_epoch'],\n",
    "        'model_parameters': exp['model_parameters'],\n",
    "        'status': exp['status'],\n",
    "        'timestamp': exp.get('timestamp', 'N/A')\n",
    "    })\n",
    "\n",
    "export_df = pd.DataFrame(export_data)\n",
    "\n",
    "# Save to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = engine.output_dir / f'probe_comparison_{timestamp}.csv'\n",
    "export_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Results exported to: {csv_path}\")\n",
    "print(f\"  Total experiments: {len(export_df)}\")\n",
    "print(f\"  Columns: {', '.join(export_df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nPreview:\")\n",
    "print(export_df[['name', 'probe_type', 'top_1_accuracy', 'training_time_seconds']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cross-Fidelity Validation (Optional)\n",
    "\n",
    "The cross-fidelity validation campaign tests how results transfer across different system scales. This is useful for:\n",
    "- Validating small-scale prototypes before full deployment\n",
    "- Understanding scaling behavior\n",
    "- Reducing computational costs during initial exploration\n",
    "\n",
    "**Setup Instructions:**\n",
    "```python\n",
    "# First, run low-fidelity experiments (fast)\n",
    "low_fidelity = engine.search(\"../configs/search_spaces/cross_fidelity_validation.yaml\")\n",
    "\n",
    "# Then analyze fidelity gap\n",
    "analyzer = ResultAnalyzer(\"results.db\")\n",
    "fidelity_gap = analyzer.compute_fidelity_gap(\n",
    "    low_fidelity_campaign='quick_test',\n",
    "    high_fidelity_campaign='full_search'\n",
    ")\n",
    "\n",
    "# Plot correlation\n",
    "engine.plot_fidelity_correlation(fidelity_gap)\n",
    "```\n",
    "\n",
    "This analysis shows whether rankings from small-scale tests match full-scale results, allowing you to predict performance without expensive computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for cross-fidelity analysis\n",
    "print(\"\\n‚ÑπÔ∏è  Cross-fidelity validation requires running both low and high fidelity campaigns.\")\n",
    "print(\"   See the markdown cell above for setup instructions.\")\n",
    "print(\"   This is an advanced feature typically used for large-scale studies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you've learned to:\n",
    "- ‚úì List and explore available search space configurations\n",
    "- ‚úì Execute multi-experiment campaigns\n",
    "- ‚úì Compute summary statistics across random seeds\n",
    "- ‚úì Generate publication-quality comparison plots\n",
    "- ‚úì Visualize training dynamics and convergence\n",
    "- ‚úì Export results to CSV for external analysis\n",
    "\n",
    "### Key Findings (Probe Comparison):\n",
    "Based on typical results, you should observe:\n",
    "1. **Structured probes** (DFT, Hadamard) outperform random baselines\n",
    "2. **Quasi-random probes** (Sobol, Halton) offer good middle ground\n",
    "3. **Variance** across seeds is typically 1-3% for stable probes\n",
    "4. **Training speed** is similar across probe types\n",
    "\n",
    "### Next Steps:\n",
    "- **Model Comparison**: Run `model_comparison.yaml` to test different architectures\n",
    "- **Hyperparameter Tuning**: Modify configs to explore learning rates, batch sizes\n",
    "- **Deep Analysis**: Use `04_analyze_results.ipynb` for statistical tests\n",
    "- **Production**: Deploy best configuration for your specific use case\n",
    "\n",
    "### Available for Exploration:\n",
    "```python\n",
    "# Quick experiments\n",
    "engine.search(\"../configs/search_spaces/quick_test.yaml\")  # 2 min\n",
    "\n",
    "# Comprehensive studies\n",
    "engine.search(\"../configs/search_spaces/model_comparison.yaml\")  # 10 min\n",
    "engine.search(\"../configs/search_spaces/sparsity_sweep.yaml\")  # 30 min\n",
    "engine.search(\"../configs/search_spaces/full_search.yaml\")  # hours\n",
    "```\n",
    "\n",
    "All results are automatically saved to `results.db` and can be analyzed anytime! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
