# ============================================================================
# Model Architecture Comparison Search Space
# ============================================================================
# Comprehensive comparison of 6 neural network architectures for RIS beam
# prediction, each with architecture-specific hyperparameters.
#
# Research Question: Which neural network architecture best captures the
# relationship between probe measurements and optimal beam indices?
#
# Total Experiments: 18 (6 models Ã— 3 seeds)
# ============================================================================

name: "model_comparison"
description: "Compare 6 neural network architectures for beam prediction"
strategy: "grid_search"

# Fixed system configuration
system:
  N: 64
  K: 64
  M: 8
  frequency: 28e9
  snr_db: 20.0

# Fixed best probe from probe_comparison
probes:
  type: "hadamard"

# Model architectures to compare
models:
  - type: "mlp"
    hidden_dims: [256, 128, 64]
    activation: "relu"
    dropout: 0.1
    description: "Baseline feed-forward network"

  - type: "cnn_1d"
    num_filters: [64, 128, 64]
    kernel_size: 3
    pooling: "max"
    dropout: 0.1
    description: "1D CNN for sequential measurement patterns"

  - type: "cnn_2d"
    num_filters: [32, 64, 32]
    kernel_size: 3
    reshape_to: [8, 8]  # Reshape M measurements to 2D spatial grid
    dropout: 0.1
    description: "2D CNN for spatial RIS element patterns"

  - type: "lstm"
    hidden_size: 128
    num_layers: 2
    bidirectional: true
    dropout: 0.2
    description: "LSTM for temporal/sequential dependencies"

  - type: "transformer"
    d_model: 128
    nhead: 4
    num_layers: 3
    dim_feedforward: 256
    dropout: 0.1
    description: "Transformer for attention-based learning"

  - type: "set_transformer"
    d_model: 128
    num_heads: 4
    num_encoder_blocks: 2
    num_decoder_blocks: 1
    dropout: 0.1
    description: "Set Transformer for permutation-invariant learning"

# Training configuration
training:
  learning_rate: 0.001
  batch_size: 64
  max_epochs: 100
  early_stopping_patience: 15
  optimizer: "adam"
  loss_function: "cross_entropy"
  val_split: 0.15
  test_split: 0.15
  scheduler: "cosine"

# Data generation
data:
  num_samples: 10000
  data_source: "synthetic"

# Random seeds
random_seeds: [42, 43, 44]

# Metrics to track
metrics:
  primary: "top_1_accuracy"
  secondary:
    - "top_5_accuracy"
    - "val_loss"
    - "num_parameters"
    - "inference_time"
    - "training_time"
    - "memory_usage"

# Scientific rules
rules:
  early_stopping: true
  abandon_threshold: 0.016
  min_epochs: 10
  prefer_simpler_models: true  # Prune if similar accuracy but 2x params

# Analysis directives
analysis:
  plot_pareto_frontier: true  # Accuracy vs. parameters
  statistical_significance: true
  ablation_study: false  # Set true to ablate components
