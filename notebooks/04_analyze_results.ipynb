{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIS Auto-Research Engine - Results Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze past experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ris_research_engine import RISEngine\n",
    "import pandas as pd\n",
    "\n",
    "engine = RISEngine(db_path=\"results.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. View Experiment History\n",
    "\n",
    "Load and display recent experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all recent experiments\n",
    "history_df = engine.show_history(limit=50)\n",
    "display(history_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Probe Types\n",
    "\n",
    "Analyze and compare different probe strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probe comparison analysis\n",
    "probe_comparison = engine.analyzer.compare_probes()\n",
    "display(probe_comparison)\n",
    "\n",
    "# Generate bar plot\n",
    "engine.reporter.probe_comparison_bar(probe_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Model Architectures\n",
    "\n",
    "Analyze different model architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison analysis\n",
    "model_comparison = engine.analyzer.compare_models()\n",
    "display(model_comparison)\n",
    "\n",
    "# Generate comparison plot\n",
    "engine.reporter.model_comparison_bar(model_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sparsity Analysis\n",
    "\n",
    "Analyze how performance varies with M (sensing budget):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity analysis\n",
    "sparsity_df = engine.analyzer.sparsity_analysis()\n",
    "display(sparsity_df)\n",
    "\n",
    "# Generate sparsity curve\n",
    "engine.reporter.sparsity_curve(sparsity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Configurations\n",
    "\n",
    "Find and analyze top performing configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 configurations\n",
    "best_configs = engine.analyzer.best_configuration(top_k=10)\n",
    "\n",
    "print(\"Top 10 Configurations:\")\n",
    "for i, result in enumerate(best_configs):\n",
    "    print(f\"{i+1}. {result.config.name}\")\n",
    "    print(f\"   Probe: {result.config.probe_type}, Model: {result.config.model_type}\")\n",
    "    print(f\"   Accuracy: {result.primary_metric_value:.4f}\")\n",
    "    print(f\"   Parameters: {result.model_parameters:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary\n",
    "\n",
    "Get comprehensive statistical summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical summary\n",
    "summary = engine.analyzer.statistical_summary()\n",
    "\n",
    "print(f\"Total experiments: {summary['total_experiments']}\")\n",
    "print(f\"\\nMetric Statistics:\")\n",
    "for metric_name, stats in summary['metrics'].items():\n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.4f}\")\n",
    "    print(f\"  Std: {stats['std']:.4f}\")\n",
    "    print(f\"  Min: {stats['min']:.4f}\")\n",
    "    print(f\"  Max: {stats['max']:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"  Mean time: {summary['training']['mean_time_seconds']:.2f}s\")\n",
    "print(f\"  Total time: {summary['training']['total_time_seconds']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Advanced Plots\n",
    "\n",
    "Create additional visualization plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all results\n",
    "all_results = engine.result_tracker.query(status='completed', limit=100)\n",
    "\n",
    "if all_results:\n",
    "    # Heatmap of probe-model combinations\n",
    "    engine.reporter.heatmap_probe_model(all_results)\n",
    "    \n",
    "    # Pareto front\n",
    "    engine.reporter.pareto_front(all_results)\n",
    "    \n",
    "    # Distribution\n",
    "    engine.reporter.ranking_distribution(all_results)\n",
    "    \n",
    "    print(f\"\\nPlots saved to: {engine.reporter.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Curves\n",
    "\n",
    "Plot training curves for best configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best result\n",
    "if best_configs:\n",
    "    best = best_configs[0]\n",
    "    engine.reporter.training_curves(best)\n",
    "    engine.reporter.baseline_comparison(best)\n",
    "    \n",
    "    print(f\"Training curves for: {best.config.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Campaign-Specific Analysis\n",
    "\n",
    "Analyze a specific campaign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify campaign name\n",
    "campaign_name = 'probe_comparison'  # Change to your campaign name\n",
    "\n",
    "# Get campaign results\n",
    "campaign_results = engine.result_tracker.query(campaign_name=campaign_name)\n",
    "\n",
    "if campaign_results:\n",
    "    print(f\"Campaign: {campaign_name}\")\n",
    "    print(f\"Total experiments: {len(campaign_results)}\")\n",
    "    \n",
    "    # Campaign-specific analysis\n",
    "    campaign_probe_comparison = engine.analyzer.compare_probes(\n",
    "        campaign_name=campaign_name\n",
    "    )\n",
    "    display(campaign_probe_comparison)\n",
    "else:\n",
    "    print(f\"No results found for campaign: {campaign_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
